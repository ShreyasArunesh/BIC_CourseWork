{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dbc05fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n *\\n * Name                   :   F21-BC Coursework-1 \\n *\\n * Description            :   it implements the following classes\\n *\\n *                            1. FCLayers\\n *                            2. MLP\\n *                            \\n *\\n * Author                 :   Shreyas Arunesh\\n *                                       \\n *\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    " *\n",
    " * Name                   :   F21-BC Coursework-1 \n",
    " *\n",
    " * Description            :   it implements the following classes\n",
    " *\n",
    " *                            1. FCLayers\n",
    " *                            2. MLP\n",
    " *                            \n",
    " *\n",
    " * Author                 :   Shreyas Arunesh\n",
    " *                                       \n",
    " *\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b78596b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''importing packages'''\n",
    "import math\n",
    "import numpy as np\n",
    "from time import time\n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ccd6eaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements relu function\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns maximium of 'x' or '0' \n",
    "     *\n",
    "     '''\n",
    "    return np.maximum(x,0)\n",
    "     \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def d_relu(x):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements derivative of relu function\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns derivative of relu function.  \n",
    "     *\n",
    "     '''\n",
    "     \n",
    "    return np.greater(x, 0).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "def tanh(x):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements hyperbolic tangent function\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns hyperbolic tangent of 'x' \n",
    "     *\n",
    "     '''\n",
    "     \n",
    "    return np.tanh(x)\n",
    "\n",
    "\n",
    "\n",
    "def d_tanh(x):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements derivative of hyperbolic tangent function\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns derivative of hyperbolic tangent of 'x' \n",
    "     *\n",
    "     ''' \n",
    "    return 1 - np.tanh(x) ** 2\n",
    "\n",
    "\n",
    "\n",
    "def sigmoid(x): \n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements sigmoid function\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns sigmoid of 'x' \n",
    "     *\n",
    "     '''\n",
    "    return np.array([1 / (1 + math.exp(-i)) for i in x])\n",
    "\n",
    "\n",
    "\n",
    "def d_sigmoid(x):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements the derivative of sigmoid function\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns derivative of sigmoid of 'x' \n",
    "     *\n",
    "     '''\n",
    "    return sigmoid(x) * 1-sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d38c32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function and its derivative\n",
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements Mean Square Error loss function.\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns mse for true and predicted value' \n",
    "     *\n",
    "     '''\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "\n",
    "\n",
    "def d_mse(y_true, y_pred):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements derivative of Mean Square Error loss function.\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns derivative of mse for true and predicted value' \n",
    "     *\n",
    "     '''\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "\n",
    "\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements Binary_Cross_Entropy loss function.\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns binary cross entropy for true and predicted value' \n",
    "     *\n",
    "    '''\n",
    "    if y_true == 1:\n",
    "        return - np.log(y_pred)\n",
    "    else:\n",
    "        return - np.log(1-y_pred)\n",
    "    \n",
    "    \n",
    "\n",
    "def d_binary_cross_entropy(y_true, y_pred):\n",
    "    '''\n",
    "     *\n",
    "     *  Summary : this block implements derivative of Binary_Cross_Entropy loss function.\n",
    "     *\n",
    "     *  Args    : Param - input 'x'\n",
    "     *\n",
    "     *  Returns : it returns derivative of binary cross entropy for true and predicted value' \n",
    "     *\n",
    "    '''\n",
    "    if y_true == 1:\n",
    "        return np.array(-1/ y_pred)\n",
    "    else:\n",
    "        return np.array(1/(1-y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a25b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inherit from base class Layer\n",
    "class FCLayer:\n",
    "    \"\"\"\n",
    "    *\n",
    "    * Summary :    This class implements a single layer of the Network.\n",
    "    *\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, output_size,activation):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : this block implements the constructor for the class FCLAyer and assigns random values \n",
    "                       the weights and bias depending on the input size. When the shape of the layer is modified, the weights \n",
    "         *            are modified too to keep the layer fully connected.\n",
    "         *\n",
    "         *  Args    : output_size        -    this gives the output size of that layer\n",
    "                                               which is the input size for next layer \n",
    "         *\n",
    "         *            input_size         -              int         - Should be provided for the first layer of the \n",
    "         *                                                           perceptron It gives the expected input shape. \n",
    "         *                                                           This is automatically computed for all the other \n",
    "         *                                                           layers.\n",
    "         *             activation        -    Activation function   - it defines the activation function\n",
    "         *\n",
    "         *\n",
    "         *  Returns : no return value\n",
    "         *\n",
    "        '''\n",
    "        self.weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self.bias = np.random.rand(1, output_size) - 0.\n",
    "        if activation == \"relu\":\n",
    "            self.activation = relu\n",
    "            self.activation_prime = d_relu\n",
    "        if activation == \"sigmoid\":\n",
    "            self.activation = sigmoid\n",
    "            self.activation_prime = d_sigmoid\n",
    "        if activation == \"tanh\":\n",
    "            self.activation = tanh\n",
    "            self.activation_prime = d_tanh\n",
    "        \n",
    "\n",
    "            \n",
    "    def forward_propagation(self, input_data):\n",
    "        '''\n",
    "     *\n",
    "     *  Summary : this block implements forward propagation.\n",
    "     *\n",
    "     *  Args    : input data\n",
    "     *\n",
    "     *  Returns : returns activation ouput. \n",
    "     *\n",
    "     '''\n",
    "        self.input = input_data\n",
    "        self.neuron_output = np.dot(self.input, self.weights) + self.bias\n",
    "        self.activation_output = self.activation(self.neuron_output)\n",
    "        return self.activation_output\n",
    "\n",
    "    \n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : this block implements back-propagation by updating the weights and bias. This computes\n",
    "                      dE/dW, dE/dB for a given output_error=dE/dY.\n",
    "\n",
    "         *  Args    : output_error and learning rate (provided byt the user)\n",
    "         *\n",
    "         *  Returns : Returns input_error=dE/dX.\n",
    "         *\n",
    "         '''\n",
    "        activation_error = self.activation_prime(self.neuron_output) * output_error\n",
    "        input_error = np.dot(activation_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, activation_error)\n",
    "        # dBias = output_error\n",
    "\n",
    "        # update parameters\n",
    "        self.weights -= learning_rate * weights_error\n",
    "        self.bias -= learning_rate * activation_error\n",
    "        return input_error\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34851318",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    \"\"\"\n",
    "    *\n",
    "    * Summary :    This class implements a Multilayer Neural network.\n",
    "    *\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : This block implements the constructor for the class \n",
    "         *\n",
    "         *  Args    : No Arguments\n",
    "         *\n",
    "         *  Returns : No return value\n",
    "         *\n",
    "        '''\n",
    "            \n",
    "        self.layers = []\n",
    "        self.loss = None\n",
    "        self.d_loss = None\n",
    "        \n",
    "\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : This block adds a layer to the neural network.\n",
    "         *\n",
    "         *  Args    : new_layer  -  Layer (class)  -  This holds an object of the class 'FCLayer'.                                          \n",
    "         *\n",
    "         *  Returns : No return value\n",
    "         *\n",
    "        '''\n",
    "        self.layers.append(layer)\n",
    "\n",
    "\n",
    "        \n",
    "    def compile(self, loss):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : This block sets the loss function to use.\n",
    "         *\n",
    "         *  Args    :loss function.                                          \n",
    "         *\n",
    "         *  Returns : No return value\n",
    "         *\n",
    "        '''\n",
    "        if loss == 'mse':\n",
    "            self.loss = mse\n",
    "            self.d_loss = d_mse\n",
    "        if loss == 'binary_cross_entropy':\n",
    "            self.loss = binary_cross_entropy\n",
    "            self.d_loss = d_binary_cross_entropy\n",
    "            \n",
    "            \n",
    "\n",
    "    def predict(self, input_data):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : This function predicts the output for the given input.\n",
    "         *\n",
    "         *  Args    : input data.                                        \n",
    "         *\n",
    "         *  Returns : returns predicted output\n",
    "         *\n",
    "        '''\n",
    "        # sample dimension first\n",
    "        result = []\n",
    "        # run network over all samples\n",
    "        for i in range(len(input_data)):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward_propagation(output)\n",
    "            result.append(output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    \n",
    "    \n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        '''\n",
    "         *\n",
    "         *  Summary : This function trains the Neural network for the given data.\n",
    "         *\n",
    "         *  Args    : x_train    -  The input to the neurons.\n",
    "                      y_train    -  The actual output of the network.\n",
    "                      Epochs     -  number of passes of the entire training dataset (Number provided by user)\n",
    "         *\n",
    "         *  Returns : No return value\n",
    "         *\n",
    "        '''\n",
    "\n",
    "        # training loop\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            for j in range(len(x_train)):\n",
    "                # forward propagation\n",
    "                output = x_train[j]\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward_propagation(output)\n",
    "\n",
    "                # compute loss (for display purpose only)\n",
    "\n",
    "                err += self.loss(y_train[j], output)\n",
    "\n",
    "                # backward propagation\n",
    "                error = np.array(self.d_loss(y_train[j], output)).reshape(1, 1)\n",
    "\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward_propagation(error, learning_rate)\n",
    "            err /= len(x_train)\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb6114d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the input data as x-train and y_train\n",
    "\n",
    "input_array = np.loadtxt(\"dataset/data_banknote_authentication.txt\",delimiter=',')\n",
    "np.random.shuffle(input_array)\n",
    "x_train = input_array[:,:-1]\n",
    "x_train = np.reshape(x_train,(-1,1,4))\n",
    "y_train = input_array[:,-1]\n",
    "y_train = np.reshape(y_train,(-1,1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc042cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Output_file = open(\"output.txt\",\"w+\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "722f44d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = [\n",
    "     # Accuracy vs layers and neurone [1-10][2-10]\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":2,\"no_neurons\":2,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":3,\"no_neurons\":3,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":4,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":5,\"no_neurons\":5,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":6,\"no_neurons\":6,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":7,\"no_neurons\":7,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":8,\"no_neurons\":8,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":9,\"no_neurons\":9,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":10,\"no_neurons\":10,\"loss\":\"mse\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "\n",
    "\n",
    "    # Accuracy vs activation function in hidden layer [tanh,relu]\n",
    "    {\"hl_act\": \"relu\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "\n",
    "    # Accuracy vs activation function in output layer [tanh,sigmoid,relu]\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "\n",
    "    # Accuracy vs learning rate [0.1-0.9]\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.2},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.3},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.4},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.5},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.6},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.7},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.8},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.9},\n",
    "\n",
    "    # Accuracy vs epoch [1-1000,100]\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":200,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":400,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":500,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":700,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":800,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":900,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "\n",
    "    # Accuracy vs loss [mse,log]\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"relu\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"tanh\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"mse\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "    {\"hl_act\": \"tanh\",\"ol_act\": \"sigmoid\",\"no_hiddenlayers\":3,\"no_neurons\":4,\"loss\":\"binary_cross_entropy\",\"epochs\":1000,\"learning_rate\":0.1},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be747573",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acccuracy = []\n",
    "avg_total_time = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69966ac1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on set :  {'hl_act': 'tanh', 'ol_act': 'relu', 'no_hiddenlayers': 2, 'no_neurons': 2, 'loss': 'mse', 'epochs': 500, 'learning_rate': 0.1}\n",
      "\n",
      "Random Weights Training subset 1 Processing...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "    *\n",
    "    * Summary : this block carries out experiments on different hyper-parameters.\n",
    "    *           1. hl_act, 2. ol_act, 3. no_hiddenlayers, 4. no_neurons\n",
    "    *           5. loss, 6. epochs, 7. learning_rate\n",
    "    *\n",
    "    * The model performance if measured over 1. Average Accuracy and 2. time taken for interation.\n",
    "    *  \n",
    "    '''\n",
    "\n",
    "# looping over all the sets in the dictionary Parameters.\n",
    "for set_index,set in enumerate(hyperparameters):\n",
    "    \n",
    "    print(f\"Training on set :  {set}\\n\")\n",
    "    Output_file.write(f\"Training on set :  {set}\\n\\n\")\n",
    "    \n",
    "    acc_set_10 = []\n",
    "    total_time_10 = []\n",
    "    \n",
    "#     Loop over 10 times for a perticular set. Each time it initialises random weights and biases.  \n",
    "    for i in range(1):\n",
    "        # start time\n",
    "        start = time()\n",
    "        \n",
    "        print(f\"Random Weights Training subset {i + 1} Processing...\\n\")\n",
    "        \n",
    "#         creating an object named model for the class MLP()\n",
    "        model = MLP()\n",
    "    \n",
    "#     adding the layers for the inputs.\n",
    "        model.add_layer(FCLayer(x_train.shape[-1], set[\"no_neurons\"], set[\"hl_act\"]))\n",
    "    \n",
    "#     Initialising the model and training with the data provided.\n",
    "        for l in range(set[\"no_hiddenlayers\"]):\n",
    "            model.add_layer(FCLayer(set[\"no_neurons\"], set[\"no_neurons\"], set[\"hl_act\"]))\n",
    "        model.add_layer(FCLayer(set[\"no_neurons\"], 1, set[\"ol_act\"]))\n",
    "        model.compile(set[\"loss\"])\n",
    "        model.fit(x_train=x_train, y_train=y_train,epochs=set[\"epochs\"], learning_rate=set[\"learning_rate\"])\n",
    "        \n",
    "#       Predicting the output for the given input.\n",
    "        pred = model.predict(x_train)\n",
    "\n",
    "        # end time\n",
    "        total_time_10.append (time() - start)\n",
    "        \n",
    "#       calculating the accuracy. \n",
    "        acc_set_10.append(np.around(pred, 0) == y_train)\n",
    "        Output_file.write(f\"Random Weights Training subset {i + 1} Processed \\n\")\n",
    "\n",
    "#    Appending the avarage accuracy score and time taken for 10 iterations to the list.\n",
    "    avg_acccuracy.append({f\"set-{set_index}\":np.mean(acc_set_10)})\n",
    "    avg_total_time.append({f\"set-{set_index}\":np.mean(total_time_10)})\n",
    "\n",
    "    avg_acccuracy.append({f\"set-{set_index}\":np.mean(acc_set_10)})\n",
    "    avg_total_time.append({f\"set-{set_index}\":np.mean(total_time_10)})\n",
    "\n",
    "    print(f\"\\nAverage Accuracy of a sets over 10 Training (random weights) : {avg_acccuracy[-1]}\")\n",
    "    Output_file.write(f\"\\nAverage Accuracy of a sets over 10 Training (random weights) : {avg_acccuracy[-1]}\")\n",
    "    print( f\"\\nAverage Time taken to run that sets over 10 Training (random weights) : {avg_total_time[-1]} seconds \\n\\n\")\n",
    "    Output_file.write( f\"\\nAverage Time taken to run that sets over 10 Training (random weights) : {avg_total_time[-1]} seconds \\n\\n\")\n",
    "\n",
    "\n",
    "Output_file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d93b162",
   "metadata": {},
   "source": [
    "# Visualisation of Model performance for diffrent Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff6e3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_no_hiddenlayers_neurons =[]\n",
    "for set in hyperparameters[:9]:\n",
    "    X_no_hiddenlayers_neurons.append(set[\"no_hiddenlayers\"])\n",
    "print(X_no_hiddenlayers_neurons)\n",
    "\n",
    "Y_accuracy = []\n",
    "Y_Avg_time = []\n",
    "for set in avg_acccuracy[:9]:\n",
    "    for key, value in set.items():\n",
    "        Y_accuracy.append(np.around(value, 2))\n",
    "for set in avg_total_time[:9]:\n",
    "    for key, value in set.items():\n",
    "        Y_Avg_time.append(np.around(value, 2))\n",
    "print(Y_accuracy)\n",
    "print (Y_Avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb02228",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_no_hiddenlayers_neurons, Y_accuracy, color = 'red')\n",
    "plt.plot(X_no_hiddenlayers_neurons, Y_accuracy, color = 'blue' )\n",
    "plt.title('Accuracy vs hidden layers and neurone')\n",
    "plt.xlabel('Number Hidden layers and neurons')\n",
    "plt.ylabel('Average accuracy for iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_no_hiddenlayers_neurons, Y_Avg_time, color = 'red')\n",
    "plt.plot(X_no_hiddenlayers_neurons, Y_Avg_time, color = 'blue' )\n",
    "plt.title('Time taken for the interation vs hidden layers and neurone')\n",
    "plt.xlabel('Number Hidden layers and neurons')\n",
    "plt.ylabel('Average time for iteration in (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da180c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hl_act =[]\n",
    "for set in hyperparameters[9:11]:\n",
    "    X_hl_act.append(set[\"hl_act\"])\n",
    "print(X_hl_act)\n",
    "\n",
    "Y_accuracy = []\n",
    "Y_Avg_time = []\n",
    "for set in avg_acccuracy[9:11]:\n",
    "    for key, value in set.items():\n",
    "        Y_accuracy.append(np.around(value, 2))\n",
    "for set in avg_total_time[9:11]:\n",
    "    for key, value in set.items():\n",
    "        Y_Avg_time.append(np.around(value, 2))\n",
    "print(Y_accuracy)\n",
    "print (Y_Avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595a29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_hl_act, Y_accuracy, color = 'red')\n",
    "plt.plot(X_no_hiddenlayers_neurons, Y_accuracy, color = 'blue' )\n",
    "plt.title('Accuracy vs Hidden layers ativation function')\n",
    "plt.xlabel('Hidden layers ativation function')\n",
    "plt.ylabel('Average accuracy for iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_hl_act, Y_Avg_time, color = 'red')\n",
    "plt.plot(X_no_hiddenlayers_neurons, Y_Avg_time, color = 'blue' )\n",
    "plt.title('Time taken for the interation vs Hidden layers ativation function')\n",
    "plt.xlabel('Hidden layers ativation function')\n",
    "plt.ylabel('Average time for iteration in (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9713ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ol_act =[]\n",
    "for set in hyperparameters[11:14]:\n",
    "    X_ol_act.append(set[\"ol_act\"])\n",
    "print(X_ol_act)\n",
    "\n",
    "Y_accuracy = []\n",
    "Y_Avg_time = []\n",
    "for set in avg_acccuracy[11:14]:\n",
    "    for key, value in set.items():\n",
    "        Y_accuracy.append(np.around(value, 2))\n",
    "for set in avg_total_time[11:14]:\n",
    "    for key, value in set.items():\n",
    "        Y_Avg_time.append(np.around(value, 2))\n",
    "print(Y_accuracy)\n",
    "print (Y_Avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c93c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_ol_act, Y_accuracy, color = 'red')\n",
    "plt.plot(X_ol_act, Y_accuracy, color = 'blue' )\n",
    "plt.title('Accuracy vs output layer ativation function')\n",
    "plt.xlabel('output layer ativation function')\n",
    "plt.ylabel('Average accuracy for iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_ol_act, Y_Avg_time, color = 'red')\n",
    "plt.plot(X_ol_act, Y_Avg_time, color = 'blue' )\n",
    "plt.title('Time taken for the interation vs output layer ativation function')\n",
    "plt.xlabel('output layers ativation function')\n",
    "plt.ylabel('Average time for iteration in (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9277a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_learning_rate =[]\n",
    "for set in hyperparameters[14:23]:\n",
    "    X_learning_rate.append(set[\"learning_rate\"])\n",
    "print(X_learning_rate)\n",
    "\n",
    "Y_accuracy = []\n",
    "Y_Avg_time = []\n",
    "for set in avg_acccuracy[14:23]:\n",
    "    for key, value in set.items():\n",
    "        Y_accuracy.append(np.around(value, 2))\n",
    "for set in avg_total_time[14:23]:\n",
    "    for key, value in set.items():\n",
    "        Y›_Avg_time.append(np.around(value, 2))\n",
    "print(Y_accuracy)\n",
    "print (Y_Avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823a8777",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_learning_rate, Y_accuracy, color = 'red')\n",
    "plt.plot(X_learning_rate, Y_accuracy, color = 'blue' )\n",
    "plt.title('Accuracy vs learning_rate')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Average accuracy for iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_learning_rate, Y_Avg_time, color = 'red')\n",
    "plt.plot(X_learning_rate, Y_Avg_time, color = 'blue' )\n",
    "plt.title('Time taken for the interation vs learning_rate')\n",
    "plt.xlabel('learning_rate')\n",
    "plt.ylabel('Average time for iteration in (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dd48b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_epochs =[]\n",
    "for set in hyperparameters[23:29]:\n",
    "    X_epochs.append(set[\"epochs\"])\n",
    "print(X_epochs)\n",
    "\n",
    "Y_accuracy = []\n",
    "Y_Avg_time = []\n",
    "for set in avg_acccuracy[23:29]:\n",
    "    for key, value in set.items():\n",
    "        Y_accuracy.append(np.around(value, 2))\n",
    "for set in avg_total_time[23:30]:\n",
    "    for key, value in set.items():\n",
    "        Y_Avg_time.append(np.around(value, 2))\n",
    "print(Y_accuracy)\n",
    "print (Y_Avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f712d88",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X_epochs, Y_accuracy, color = 'red')\n",
    "plt.plot(X_epochs, Y_accuracy, color = 'blue' )\n",
    "plt.title('Accuracy vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Average accuracy for iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_epochs, Y_Avg_time, color = 'red')\n",
    "plt.plot(X_epochs, Y_Avg_time, color = 'blue' )\n",
    "plt.title('Time taken for the interation vs epochs')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('Average time for iteration in (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af488b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_loss =[]\n",
    "for set in hyperparameters[30:]:\n",
    "    X_loss.append(set[\"loss\"])\n",
    "print(X_loss)\n",
    "\n",
    "Y_accuracy = []\n",
    "Y_Avg_time = []\n",
    "for set in avg_acccuracy[29:]:\n",
    "    for key, value in set.items():\n",
    "        Y_accuracy.append(np.around(value, 2))\n",
    "for set in avg_total_time[23:29]:\n",
    "    for key, value in set.items():\n",
    "        Y_Avg_time.append(np.around(value, 2))\n",
    "print(Y_accuracy)\n",
    "print (Y_Avg_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dedff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_loss, Y_accuracy, color = 'red')\n",
    "plt.plot(X_loss, Y_accuracy, color = 'blue' )\n",
    "plt.title('Accuracy vs loss')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('Average accuracy for iteration')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(X_epochs, Y_Avg_time, color = 'red')\n",
    "plt.plot(X_epochs, Y_Avg_time, color = 'blue' )\n",
    "plt.title('Time taken for the interation vs epochs')\n",
    "plt.xlabel('loss')\n",
    "plt.ylabel('Average time for iteration in (seconds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b733d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
